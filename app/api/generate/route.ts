import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { prisma } from "@/lib/prisma";
import { HfInference } from "@huggingface/inference";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

export async function POST(request: NextRequest) {
  try {
    // 🔐 Auth check
    const session = await getServerSession(authOptions);
    if (!session?.user?.email) {
      return NextResponse.json(
        { error: "Unauthorized. Please log in first." },
        { status: 401 }
      );
    }

    // 👤 User lookup
    const user = await prisma.user.findUnique({
      where: { email: session.user.email },
    });

    if (!user) {
      return NextResponse.json(
        { error: "User not found. Please log in again." },
        { status: 404 }
      );
    }

    if (user.credits <= 0) {
      return NextResponse.json(
        { error: "You're out of credits!", credits: 0 },
        { status: 403 }
      );
    }

    // 📥 Request body
    const { prompt, type, tone, length } = await request.json();

    if (!prompt?.trim()) {
      return NextResponse.json(
        { error: "Please enter a prompt." },
        { status: 400 }
      );
    }

    const hfKey = process.env.HUGGINGFACE_API_KEY ||
      process.env.HF_TOKEN ||
      process.env.HF_API_KEY ||
      process.env.AI_TOKEN;

    if (!hfKey) {
      return NextResponse.json(
        { error: "Hugging Face API key is not configured. We checked HUGGINGFACE_API_KEY, HF_TOKEN, HF_API_KEY, and AI_TOKEN." },
        { status: 500 }
      );
    }

    const hf = new HfInference(hfKey);

    const systemPrompt = `You are a professional content writer. Generate ${type || "blog"} content in a ${tone || "professional"} tone. 
Make it ${length || "medium"} length. Be creative, engaging, and informative. Do not repeat the instruction in your output.`;

    const maxTokens = length === "short" ? 300 : length === "medium" ? 600 : 1000;

    const response = await hf.chatCompletion({
      model: "mistralai/Mistral-7B-Instruct-v0.2",
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: prompt }
      ],
      max_tokens: maxTokens,
      temperature: 0.7,
    });

    const content = response.choices[0]?.message?.content?.trim() ?? "";

    if (!content) {
      throw new Error("No content was generated by the model.");
    }

    // 💾 Save result
    await prisma.generation.create({
      data: {
        title: prompt.slice(0, 50),
        content,
        type: type || "blog",
        tone: tone || "professional",
        length: length || "medium",
        userId: user.id,
      },
    });

    // 💳 Deduct credit
    const updatedUser = await prisma.user.update({
      where: { id: user.id },
      data: { credits: { decrement: 1 } },
    });

    return NextResponse.json({
      content,
      credits: updatedUser.credits,
      message: "Content generated successfully",
    });
  } catch (error: any) {
    console.error("Generation API error:", error);

    // Friendly error handling for Hugging Face limits
    if (error.message?.includes("429") || error.status === 429) {
      return NextResponse.json(
        { error: "Hugging Face is currently busy or you have reached your free limit. Please try again in an hour." },
        { status: 429 }
      );
    }

    if (error.message?.includes("503") || error.status === 503) {
      return NextResponse.json(
        { error: "The AI model is currently loading or under heavy load. Please try again in a few minutes." },
        { status: 503 }
      );
    }

    return NextResponse.json(
      { error: error.message || "Failed to generate content" },
      { status: 500 }
    );
  }
}
